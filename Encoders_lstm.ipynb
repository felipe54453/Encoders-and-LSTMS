{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNetEncoder e TransformerEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de m√≥dulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import layers\n",
    "import re\n",
    "import string\n",
    "\n",
    "import keras_nlp\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principais parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "MAX_SEQUENCE_LENGTH = 512 # Somente considerar as 512 primeiras palavras de cada review\n",
    "VOCAB_SIZE = 15000 # Somente considerar 15000 palavras\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Luckily, I didn't have to travel far to make m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Definitely come for Happy hour! Prices are ama...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nobuo shows his unique talents with everything...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  My wife took me here on my birthday for breakf...      5\n",
       "1  I have no idea why some people give bad review...      5\n",
       "2  love the gyro plate. Rice is so good and I als...      4\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4  General Manager Scott Petello is a good egg!!!...      5\n",
       "5  Quiessence is, simply put, beautiful.  Full wi...      4\n",
       "6  Drop what you're doing and drive here. After I...      5\n",
       "7  Luckily, I didn't have to travel far to make m...      4\n",
       "8  Definitely come for Happy hour! Prices are ama...      4\n",
       "9  Nobuo shows his unique talents with everything...      5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/prof-renato/data/main/yelp.csv')[['text', 'stars']]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stars'].value_counts().unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando inputs e outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].values\n",
    "y = df['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5\n",
       "0  0  0  0  0  1\n",
       "1  0  0  0  0  1\n",
       "2  0  0  0  1  0\n",
       "3  0  0  0  0  1\n",
       "4  0  0  0  0  1\n",
       "5  0  0  0  1  0\n",
       "6  0  0  0  0  1\n",
       "7  0  0  0  1  0\n",
       "8  0  0  0  1  0\n",
       "9  0  0  0  0  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para formato do tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 15:04:47.357007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:47.409642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:47.410018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5600\n",
      "Validation set size: 1400\n",
      "Testing set size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 15:04:47.411646: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-05 15:04:47.412395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:47.412717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:47.412914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:48.037923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:48.038160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:48.038348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 15:04:48.038496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3372 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©-processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "vectorizer = layers.TextVectorization(3000, standardize=custom_standardization, output_sequence_length=150)\n",
    "\n",
    "# Para um conjunto de dados muito grande, o adapt pode remover c√≥pias sobressalentes do conjunto de dados da mem√≥ria.\n",
    "vectorizer.adapt(train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE))\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = vectorizer(text)\n",
    "    return text, label\n",
    "\n",
    "# Vetoriza√ß√£o dos dados e pr√©-busca/buffer ass√≠ncrona dos dados para melhor desempenho na GPU.\n",
    "train_ds = train_dataset.batch(BATCH_SIZE).map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_dataset.batch(BATCH_SIZE).map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_dataset.batch(BATCH_SIZE).map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         65536     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 64)         41216     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 64)         24832     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,909\n",
      "Trainable params: 131,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=MAX_SEQUENCE_LENGTH, output_dim=EMBED_DIM)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(INTERMEDIATE_DIM, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(INTERMEDIATE_DIM, return_sequences=True))(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "lstm_model = keras.Model(inputs, outputs)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 15:04:57.769895: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 12s 51ms/step - loss: 1.4323 - accuracy: 0.3596 - val_loss: 1.3319 - val_accuracy: 0.3936\n",
      "Epoch 2/20\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.2613 - accuracy: 0.4371 - val_loss: 1.2390 - val_accuracy: 0.4357\n",
      "Epoch 3/20\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.1502 - accuracy: 0.4973 - val_loss: 1.2203 - val_accuracy: 0.4336\n",
      "Epoch 4/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.1137 - accuracy: 0.5143 - val_loss: 1.1922 - val_accuracy: 0.4600\n",
      "Epoch 5/20\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.0881 - accuracy: 0.5286 - val_loss: 1.2098 - val_accuracy: 0.4579\n",
      "Epoch 6/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.0582 - accuracy: 0.5418 - val_loss: 1.2187 - val_accuracy: 0.4579\n",
      "Epoch 7/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.0458 - accuracy: 0.5454 - val_loss: 1.2421 - val_accuracy: 0.4643\n",
      "Epoch 8/20\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.0283 - accuracy: 0.5548 - val_loss: 1.2246 - val_accuracy: 0.4736\n",
      "Epoch 9/20\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 1.0129 - accuracy: 0.5612 - val_loss: 1.2153 - val_accuracy: 0.4721\n",
      "Epoch 10/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 1.0110 - accuracy: 0.5609 - val_loss: 1.2530 - val_accuracy: 0.4764\n",
      "Epoch 11/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.9807 - accuracy: 0.5677 - val_loss: 1.2677 - val_accuracy: 0.4786\n",
      "Epoch 12/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.9746 - accuracy: 0.5779 - val_loss: 1.2670 - val_accuracy: 0.4771\n",
      "Epoch 13/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.9588 - accuracy: 0.5845 - val_loss: 1.2528 - val_accuracy: 0.4629\n",
      "Epoch 14/20\n",
      "88/88 [==============================] - 3s 31ms/step - loss: 0.9295 - accuracy: 0.6054 - val_loss: 1.2642 - val_accuracy: 0.4779\n",
      "Epoch 15/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.9114 - accuracy: 0.6173 - val_loss: 1.2910 - val_accuracy: 0.4836\n",
      "Epoch 16/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.9124 - accuracy: 0.6164 - val_loss: 1.3222 - val_accuracy: 0.4614\n",
      "Epoch 17/20\n",
      "88/88 [==============================] - 3s 32ms/step - loss: 0.9084 - accuracy: 0.6204 - val_loss: 1.3396 - val_accuracy: 0.4714\n",
      "Epoch 18/20\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.9018 - accuracy: 0.6263 - val_loss: 1.3302 - val_accuracy: 0.4671\n",
      "Epoch 19/20\n",
      "88/88 [==============================] - 3s 33ms/step - loss: 0.8810 - accuracy: 0.6359 - val_loss: 1.3629 - val_accuracy: 0.4643\n",
      "Epoch 20/20\n",
      "88/88 [==============================] - 3s 34ms/step - loss: 0.8862 - accuracy: 0.6305 - val_loss: 1.4495 - val_accuracy: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2605589d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(\n",
    "    train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 15ms/step - loss: 1.4231 - accuracy: 0.4547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.4231014251708984, 'accuracy': 0.4546666741371155}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(test_ds, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativo de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=\"train + test\",\n",
    "    as_supervised=True,\n",
    "    batch_size=-1,\n",
    "    shuffle_files=False,\n",
    ")\n",
    "reviews, labels = tfds.as_numpy(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando conjuntos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 28000\n",
      "Validation set size: 7000\n",
      "Testing set size: 15000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.30, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©-processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetoriza√ß√£o dos dados e pr√©-busca/buffer ass√≠ncrona dos dados para melhor desempenho na GPU.\n",
    "train_ds = train_dataset.batch(BATCH_SIZE).map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_dataset.batch(BATCH_SIZE).map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_dataset.batch(BATCH_SIZE).map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura de modelo 1: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 128)         65536     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 64)         41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, None, 64)         24832     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, None, 64)         24832     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,481\n",
      "Trainable params: 156,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=MAX_SEQUENCE_LENGTH, output_dim=EMBED_DIM)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(INTERMEDIATE_DIM, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(INTERMEDIATE_DIM, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(INTERMEDIATE_DIM, return_sequences=True))(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "lstm_model = keras.Model(inputs, outputs)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "438/438 [==============================] - 33s 52ms/step - loss: 0.5433 - accuracy: 0.7155 - val_loss: 0.5031 - val_accuracy: 0.7536\n",
      "Epoch 2/3\n",
      "438/438 [==============================] - 20s 46ms/step - loss: 0.4797 - accuracy: 0.7686 - val_loss: 0.4947 - val_accuracy: 0.7651\n",
      "Epoch 3/3\n",
      "438/438 [==============================] - 21s 48ms/step - loss: 0.4603 - accuracy: 0.7787 - val_loss: 0.4790 - val_accuracy: 0.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2540ef2d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 5s 20ms/step - loss: 0.4585 - accuracy: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.45852214097976685, 'accuracy': 0.7825999855995178}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(test_ds, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, None, 128)        1985536   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " f_net_encoder (FNetEncoder)  (None, None, 128)        8864      \n",
      "                                                                 \n",
      " f_net_encoder_1 (FNetEncode  (None, None, 128)        8864      \n",
      " r)                                                              \n",
      "                                                                 \n",
      " f_net_encoder_2 (FNetEncode  (None, None, 128)        8864      \n",
      " r)                                                              \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,012,257\n",
      "Trainable params: 2,012,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(inputs) ## Al√©m de transformar o dados esparso em denso, a posi√ß√£o do token √© levada em conta para que dar um sentido semantico ao dado\n",
    "\n",
    "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(x)\n",
    "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(x)\n",
    "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "fnet_model = keras.Model(inputs, outputs)\n",
    "fnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "438/438 [==============================] - 24s 47ms/step - loss: 0.5370 - accuracy: 0.7021 - val_loss: 0.4496 - val_accuracy: 0.7831\n",
      "Epoch 2/3\n",
      "438/438 [==============================] - 21s 49ms/step - loss: 0.3708 - accuracy: 0.8343 - val_loss: 0.4227 - val_accuracy: 0.8061\n",
      "Epoch 3/3\n",
      "438/438 [==============================] - 20s 45ms/step - loss: 0.3028 - accuracy: 0.8721 - val_loss: 0.4674 - val_accuracy: 0.7993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1b47dadd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnet_model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 4s 17ms/step - loss: 0.4406 - accuracy: 0.8102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.44057032465934753, 'accuracy': 0.8101999759674072}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnet_model.evaluate(test_ds, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids (InputLayer)      [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, None, 128)        1985536   \n",
      " g_1 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 128)        74912     \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 128)        74912     \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_2 (Tran  (None, None, 128)        74912     \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,210,401\n",
      "Trainable params: 2,210,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_HEADS = 2\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerEncoder(intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS)(x)\n",
    "x = keras_nlp.layers.TransformerEncoder(intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS)(x)\n",
    "x = keras_nlp.layers.TransformerEncoder(intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS)(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "transformer_model = keras.Model(inputs, outputs)\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "438/438 [==============================] - 41s 84ms/step - loss: 0.4567 - accuracy: 0.7736 - val_loss: 0.4892 - val_accuracy: 0.7720\n",
      "Epoch 2/3\n",
      "438/438 [==============================] - 37s 84ms/step - loss: 0.3624 - accuracy: 0.8392 - val_loss: 0.4803 - val_accuracy: 0.7861\n",
      "Epoch 3/3\n",
      "438/438 [==============================] - 37s 83ms/step - loss: 0.3367 - accuracy: 0.8556 - val_loss: 0.5444 - val_accuracy: 0.7581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1b40a5850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 8s 33ms/step - loss: 0.5259 - accuracy: 0.7650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.5258623361587524, 'accuracy': 0.7649999856948853}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.evaluate(test_ds, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 4.6\n",
    "Aplicar FnetEncoder e TransformerEncoder na base do [yelp](https://raw.githubusercontent.com/prof-renato/data/main/yelp.csv).\n",
    "Qual modelo performa melhor? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('2tiar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a542f3dc45dd22737fc0e96588936644c18f2e4fdc7999a8003427d70f2220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
